{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model 2 seq2seq using lib",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "xO0UtYGfCuU-",
        "qI3AJND0mb0r",
        "x_xON6b4nS8x"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/theamrzaki/text_summurization_abstractive_methods/blob/master/Model_2_seq2seq_using_lib.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "HdMyi2D4-6YC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Model 2 seq2seq for text Summurization using seq2seq lib**"
      ]
    },
    {
      "metadata": {
        "id": "IzJv9UUti_og",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Intro\n",
        "This is a modification to https://github.com/dongjun-Lee/text-summarization-tensorflow \n",
        "I am builging it in a notebook envronment to be able to easily integrate with colab\n"
      ]
    },
    {
      "metadata": {
        "id": "6lSefB0pn9Gq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helpers (Googel Drive , Utilits)"
      ]
    },
    {
      "metadata": {
        "id": "xO0UtYGfCuU-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "mGpMpizWCwK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1062
        },
        "outputId": "caa1b8a6-83d0-4014-f4c6-99e3c32180bc"
      },
      "cell_type": "code",
      "source": [
        "#working google drive ya RAB\n",
        "#https://stackoverflow.com/questions/52385655/unable-to-locate-package-google-drive-ocamlfuse-suddenly-stopped-working\n",
        "\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!apt-get install -f\n",
        "!apt-get -y install -qq fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "--2018-10-21 19:22:33--  https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
            "Resolving launchpad.net (launchpad.net)... 91.189.89.223, 91.189.89.222\n",
            "Connecting to launchpad.net (launchpad.net)|91.189.89.223|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://launchpadlibrarian.net/386846978/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb [following]\n",
            "--2018-10-21 19:22:39--  https://launchpadlibrarian.net/386846978/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
            "Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 91.189.89.229, 91.189.89.228\n",
            "Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|91.189.89.229|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1232624 (1.2M) [application/x-debian-package]\n",
            "Saving to: ‘google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb’\n",
            "\n",
            "google-drive-ocamlf 100%[===================>]   1.17M  1.51MB/s    in 0.8s    \n",
            "\n",
            "2018-10-21 19:22:40 (1.51 MB/s) - ‘google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb’ saved [1232624/1232624]\n",
            "\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 22278 files and directories currently installed.)\n",
            "Preparing to unpack google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.0-0ubuntu1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-drive-ocamlfuse:\n",
            " google-drive-ocamlfuse depends on libfuse2 (>= 2.8); however:\n",
            "  Package libfuse2 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-drive-ocamlfuse (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Errors were encountered while processing:\n",
            " google-drive-ocamlfuse\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libfuse2\n",
            "Suggested packages:\n",
            "  fuse\n",
            "The following NEW packages will be installed:\n",
            "  libfuse2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 12 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 80.9 kB of archives.\n",
            "After this operation, 313 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfuse2 amd64 2.9.7-1ubuntu1 [80.9 kB]\n",
            "Fetched 80.9 kB in 0s (163 kB/s)\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 22283 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.0-0ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "(Reading database ... 22295 files and directories currently installed.)\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2pNpy7UgCx9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utilits"
      ]
    },
    {
      "metadata": {
        "id": "X0VWHFAnoDL4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/utils.py"
      ]
    },
    {
      "metadata": {
        "id": "hf4b16KopBx7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install wget\n",
        "  \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lA1tCj2mn_2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "default_path = \"drive/Colab Notebooks/Data Multcloud/Summary Data/Copy of summary.tar/\"\n",
        "\n",
        "train_article_path = default_path + \"sumdata/train/train.article.txt\"\n",
        "train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
        "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
        "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n",
        "\n",
        "\n",
        "def clean_str(sentence):\n",
        "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def get_text_list(data_path, toy):\n",
        "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if not toy:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()]\n",
        "        else:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:50]\n",
        "\n",
        "\n",
        "def build_dict(step, toy=False):\n",
        "    if step == \"train\":\n",
        "        train_article_list = get_text_list(train_article_path, toy)\n",
        "        train_title_list = get_text_list(train_title_path, toy)\n",
        "\n",
        "        words = list()\n",
        "        for sentence in train_article_list + train_title_list:\n",
        "            for word in word_tokenize(sentence):\n",
        "                words.append(word)\n",
        "\n",
        "        word_counter = collections.Counter(words).most_common()\n",
        "        word_dict = dict()\n",
        "        word_dict[\"<padding>\"] = 0\n",
        "        word_dict[\"<unk>\"] = 1\n",
        "        word_dict[\"<s>\"] = 2\n",
        "        word_dict[\"</s>\"] = 3\n",
        "        for word, _ in word_counter:\n",
        "            word_dict[word] = len(word_dict)\n",
        "\n",
        "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
        "            pickle.dump(word_dict, f)\n",
        "\n",
        "    elif step == \"valid\":\n",
        "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
        "            word_dict = pickle.load(f)\n",
        "\n",
        "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
        "\n",
        "    article_max_len = 50\n",
        "    summary_max_len = 15\n",
        "\n",
        "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
        "\n",
        "\n",
        "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
        "    if step == \"train\":\n",
        "        article_list = get_text_list(train_article_path, toy)\n",
        "        title_list = get_text_list(train_title_path, toy)\n",
        "    elif step == \"valid\":\n",
        "        article_list = get_text_list(valid_article_path, toy)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    x = [word_tokenize(d) for d in article_list]\n",
        "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
        "    x = [d[:article_max_len] for d in x]\n",
        "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
        "    \n",
        "    if step == \"valid\":\n",
        "        return x\n",
        "    else:        \n",
        "        y = [word_tokenize(d) for d in title_list]\n",
        "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
        "        y = [d[:(summary_max_len - 1)] for d in y]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
        "\n",
        "\n",
        "def get_init_embedding(reversed_dict, embedding_size):\n",
        "    #glove_file = default_path + \"glove/glove.6B.300d.txt\"\n",
        "    #word2vec_file = get_tmpfile(default_path + \"word2vec_format.vec\")\n",
        "    #glove2word2vec(glove_file, word2vec_file)\n",
        "    print(\"Loading Glove vectors...\")\n",
        "    #word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
        "\n",
        "    with open( default_path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
        "        word_vectors = pickle.load(handle)\n",
        "        \n",
        "    word_vec_list = list()\n",
        "    for _, word in sorted(reversed_dict.items()):\n",
        "        try:\n",
        "            word_vec = word_vectors.word_vec(word)\n",
        "        except KeyError:\n",
        "            word_vec = np.zeros([embedding_size], dtype=np.float32)\n",
        "\n",
        "        word_vec_list.append(word_vec)\n",
        "\n",
        "    # Assign random vector to <s>, </s> token\n",
        "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
        "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
        "\n",
        "    return np.array(word_vec_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qI3AJND0mb0r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "metadata": {
        "id": "X1B9ZgJqmsni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/prep_data.py\n",
        "\n",
        "1.   Word Embedding : Used [Glove pre-trained vectors](https://nlp.stanford.edu/projects/glove/ ) to initialize word embedding.  \n",
        "2.   Dataset :  Dataset is available at [harvardnlp/sent-summary](https://github.com/harvardnlp/sent-summary). Locate the summary.tar.gz file in project root directory.   \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "g2dpVeOqnJAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import zipfile\n",
        "import argparse\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument(\"--glove\", action=\"store_true\")\n",
        "#args = parser.parse_args()\n",
        "\n",
        "# Extract data file\n",
        "#with tarfile.open(default_path + \"sumdata/train/summary.tar.gz\", \"r:gz\") as tar:\n",
        "#    tar.extractall()\n",
        "\n",
        "with gzip.open(default_path + \"sumdata/train/train.article.txt.gz\", \"rb\") as gz:\n",
        "    with open(default_path + \"sumdata/train/train.article.txt\", \"wb\") as out:\n",
        "        out.write(gz.read())\n",
        "\n",
        "with gzip.open(default_path + \"sumdata/train/train.title.txt.gz\", \"rb\") as gz:\n",
        "    with open(default_path + \"sumdata/train/train.title.txt\", \"wb\") as out:\n",
        "        out.write(gz.read())\n",
        "\n",
        "        \n",
        "#if args.glove:\n",
        "#    glove_dir = \"glove\"\n",
        "#    glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
        "#\n",
        "#    if not os.path.exists(glove_dir):\n",
        "#        os.mkdir(glove_dir)\n",
        "#\n",
        "#    # Download glove vector\n",
        "#    wget.download(glove_url, out=glove_dir)\n",
        "#\n",
        "#    # Extract glove file\n",
        "#    with zipfile.ZipFile(os.path.join(\"glove\", \"glove.42B.300d.zip\"), \"r\") as z:\n",
        "#        z.extractall(glove_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_r1e_f2nCKK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Model:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MABp2aWMoNKh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Encoder-Decoder model with attention mechanism.\n",
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/model.py\n",
        "\n",
        "1.   Encoder : Used LSTM cell with stack_bidirectional_dynamic_rnn.\n",
        "2.   Decoder : Used LSTM BasicDecoder for training, and BeamSearchDecoder for inference.\n",
        "3.   Attention Mechanism : Used BahdanauAttention with weight normalization."
      ]
    },
    {
      "metadata": {
        "id": "GKHwTTZg5eJa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "#from utils import get_init_embedding\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
        "        self.vocabulary_size = len(reversed_dict)\n",
        "        self.embedding_size = args.embedding_size\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.num_layers = args.num_layers\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.beam_width = args.beam_width\n",
        "        if not forward_only:\n",
        "            self.keep_prob = args.keep_prob\n",
        "        else:\n",
        "            self.keep_prob = 1.0\n",
        "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
        "        with tf.variable_scope(\"decoder/projection\"):\n",
        "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
        "\n",
        "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
        "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
        "        self.X_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            if not forward_only and args.glove:\n",
        "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
        "            else:\n",
        "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
        "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
        "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
        "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
        "\n",
        "        with tf.name_scope(\"encoder\"):\n",
        "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
        "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
        "\n",
        "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
        "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
        "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
        "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
        "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
        "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
        "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
        "\n",
        "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
        "            decoder_cell = self.cell(self.num_hidden * 2)\n",
        "\n",
        "            if not forward_only:\n",
        "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
        "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
        "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
        "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
        "                self.decoder_output = outputs.rnn_output\n",
        "                self.logits = tf.transpose(\n",
        "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
        "                self.logits_reshape = tf.concat(\n",
        "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
        "            else:\n",
        "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
        "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
        "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
        "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
        "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "                    cell=decoder_cell,\n",
        "                    embedding=self.embeddings,\n",
        "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
        "                    end_token=tf.constant(3),\n",
        "                    initial_state=initial_state,\n",
        "                    beam_width=self.beam_width,\n",
        "                    output_layer=self.projection_layer\n",
        "                )\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
        "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            if not forward_only:\n",
        "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
        "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
        "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
        "\n",
        "                params = tf.trainable_variables()\n",
        "                gradients = tf.gradients(self.loss, params)\n",
        "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_xON6b4nS8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "ak3tIhETnaIJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/train.py\n",
        "\n",
        "We used sumdata/train/train.article.txt and sumdata/train/train.title.txt for training data. To train the model, use"
      ]
    },
    {
      "metadata": {
        "id": "vq9ZA0hFnUZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "98efd864-8ad8-4d71-ecc5-fe2f0e7b39c8"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.perf_counter()\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "# Uncomment next 2 lines to suppress error and Tensorflow info verbosity. Or change logging levels\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "#def add_arguments(parser):\n",
        "#    parser.add_argument(\"--num_hidden\", type=int, default=150, help=\"Network size.\")\n",
        "#    parser.add_argument(\"--num_layers\", type=int, default=2, help=\"Network depth.\")\n",
        "#    parser.add_argument(\"--beam_width\", type=int, default=10, help=\"Beam width for beam search decoder.\")\n",
        "#    parser.add_argument(\"--glove\", action=\"store_true\", help=\"Use glove as initial word embedding.\")\n",
        "#    parser.add_argument(\"--embedding_size\", type=int, default=300, help=\"Word embedding size.\")\n",
        "#\n",
        "#    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate.\")\n",
        "#    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
        "#    parser.add_argument(\"--num_epochs\", type=int, default=10, help=\"Number of epochs.\")\n",
        "#    parser.add_argument(\"--keep_prob\", type=float, default=0.8, help=\"Dropout keep prob.\")\n",
        "#\n",
        "#    parser.add_argument(\"--toy\", action=\"store_true\", help=\"Use only 50K samples of data\")\n",
        "#\n",
        "#    parser.add_argument(\"--with_model\", action=\"store_true\", help=\"Continue from previously saved model\")\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size=300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size=64\n",
        "args.num_epochs=10\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy=\"store_true\"\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#add_arguments(parser)\n",
        "#args = parser.parse_args()\n",
        "#with open(\"args.pickle\", \"wb\") as f:\n",
        "#    pickle.dump(args, f)\n",
        "\n",
        "if not os.path.exists(default_path + \"saved_model\"):\n",
        "    os.mkdir(default_path + \"saved_model\")\n",
        "else:\n",
        "    #if args.with_model:\n",
        "        old_model_checkpoint_path = open(default_path + 'saved_model/checkpoint', 'r')\n",
        "        old_model_checkpoint_path = \"\".join([default_path + \"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
        "\n",
        "\n",
        "print(\"Building dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
        "print(\"Loading training dataset...\")\n",
        "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    if 'old_model_checkpoint_path' in globals():\n",
        "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
        "        saver.restore(sess, old_model_checkpoint_path )\n",
        "\n",
        "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
        "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
        "\n",
        "    print(\"\\nIteration starts.\")\n",
        "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
        "    for batch_x, batch_y in batches:\n",
        "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
        "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
        "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
        "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
        "\n",
        "        batch_decoder_input = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
        "        batch_decoder_output = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
        "\n",
        "        train_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "            model.decoder_input: batch_decoder_input,\n",
        "            model.decoder_len: batch_decoder_len,\n",
        "            model.decoder_target: batch_decoder_output\n",
        "        }\n",
        "\n",
        "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
        "\n",
        "        if step % num_batches_per_epoch == 0:\n",
        "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
        "            minutes, seconds = divmod(rem, 60)\n",
        "            saver.save(sess, default_path + \"saved_model/model.ckpt\", global_step=step)\n",
        "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
        "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary...\n",
            "Loading training dataset...\n",
            "Loading Glove vectors...\n",
            "WARNING:tensorflow:From <ipython-input-8-3e3c149b757b>:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
            "\n",
            "Iteration starts.\n",
            "Number of batches per epoch : 782\n",
            " Epoch 1: Model is saved. Elapsed: 00:07:54.18 \n",
            "\n",
            "step 1000: loss = 42.04231643676758\n",
            " Epoch 2: Model is saved. Elapsed: 00:13:45.81 \n",
            "\n",
            "step 2000: loss = 28.758838653564453\n",
            " Epoch 3: Model is saved. Elapsed: 00:19:41.99 \n",
            "\n",
            "step 3000: loss = 26.566612243652344\n",
            " Epoch 4: Model is saved. Elapsed: 00:25:37.42 \n",
            "\n",
            " Epoch 5: Model is saved. Elapsed: 00:31:31.35 \n",
            "\n",
            "step 4000: loss = 21.597976684570312\n",
            " Epoch 6: Model is saved. Elapsed: 00:37:24.40 \n",
            "\n",
            "step 5000: loss = 17.827959060668945\n",
            " Epoch 7: Model is saved. Elapsed: 00:43:21.86 \n",
            "\n",
            "step 6000: loss = 10.550741195678711\n",
            " Epoch 8: Model is saved. Elapsed: 00:49:20.35 \n",
            "\n",
            "step 7000: loss = 12.749479293823242\n",
            " Epoch 9: Model is saved. Elapsed: 00:55:18.08 \n",
            "\n",
            " Epoch 10: Model is saved. Elapsed: 01:01:16.07 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TbbtwLe7njGS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test"
      ]
    },
    {
      "metadata": {
        "id": "iCBIdwESnlhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/test.py\n",
        "\n",
        "Generate summary of each article in sumdata/train/valid.article.filter.txt by"
      ]
    },
    {
      "metadata": {
        "id": "1IUJ0dpon1Hi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "98376749-0584-47e4-ab27-f44501bee5a7"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "\n",
        "#with open(\"args.pickle\", \"rb\") as f:\n",
        "#    args = pickle.load(f)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size=300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size=64\n",
        "args.num_epochs=10\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy=True\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "\n",
        "print(\"Loading dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
        "print(\"Loading validation dataset...\")\n",
        "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(\"Loading saved model...\")\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
        "\n",
        "    print(\"Writing summaries to 'result.txt'...\")\n",
        "    for batch_x, _ in batches:\n",
        "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
        "\n",
        "        valid_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "        }\n",
        "\n",
        "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
        "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
        "\n",
        "        with open(default_path + \"result.txt\", \"a\") as f:\n",
        "            for line in prediction_output:\n",
        "                summary = list()\n",
        "                for word in line:\n",
        "                    if word == \"</s>\":\n",
        "                        break\n",
        "                    if word not in summary:\n",
        "                        summary.append(word)\n",
        "                print(\" \".join(summary), file=f)\n",
        "\n",
        "    print('Summaries are saved to \"result.txt\"...')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dictionary...\n",
            "Loading validation dataset...\n",
            "Loading saved model...\n",
            "INFO:tensorflow:Restoring parameters from drive/Colab Notebooks/Data Multcloud/Summary Data/Copy of summary.tar/saved_model/model.ckpt-7820\n",
            "Writing summaries to 'result.txt'...\n",
            "Summaries are saved to \"result.txt\"...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tyi0SXZL0GnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}